#!/usr/bin/env python3
"""
S01E05 - Cenzura danych agentÃ³w przez LLM
Cenzuruje imiÄ™ i nazwisko, wiek, miasto oraz ulicÄ™+numer,
zastÄ™pujÄ…c je sÅ‚owem "CENZURA" wyÅ‚Ä…cznie przez LLM lub GLiNER.
ObsÅ‚uga: openai, lmstudio, anything, gemini, claude, gliner.

DODANO: Silnik GLiNER - deterministyczna cenzura NER bez LLM
        Podmiana na poziomie char-offsets = zero ryzyka zmiany reszty tekstu
"""

import argparse
import os
import re
import sys
from typing import Optional, Dict, Any, List
from abc import ABC, abstractmethod

import requests
from dotenv import load_dotenv

load_dotenv(override=True)

# StaÅ‚e dla komunikatÃ³w bÅ‚Ä™dÃ³w
MISSING_OPENAI_KEY_MSG = "âŒ Brak OPENAI_API_KEY"
MISSING_CLAUDE_KEY_MSG = "âŒ Brak CLAUDE_API_KEY lub ANTHROPIC_API_KEY w .env"
MISSING_GEMINI_KEY_MSG = "âŒ Brak GEMINI_API_KEY w .env"
UNSUPPORTED_ENGINE_MSG = "âŒ NieobsÅ‚ugiwany silnik:"
MISSING_OPENAI_INSTALL_MSG = "âŒ Musisz zainstalowaÄ‡ openai: pip install openai"
MISSING_ANTHROPIC_INSTALL_MSG = "âŒ Musisz zainstalowaÄ‡ anthropic: pip install anthropic"
MISSING_GEMINI_INSTALL_MSG = "âŒ Musisz zainstalowaÄ‡ google-generativeai: pip install google-generativeai"
MISSING_GLINER_INSTALL_MSG = "âŒ Musisz zainstalowaÄ‡ gliner: pip install gliner"

# DomyÅ›lny model GLiNER - multilingual PII, obsÅ‚uguje polski
# Alternatywy do przetestowania (lepszy F1, ale wymaga pobrania):
#   "knowledgator/gliner-pii-base-v1.0"    - najwyÅ¼szy F1 (80.99%), wymaga pip install gliner
#   "urchade/gliner_large-v2.1"             - bazowy large, dobry dla wÅ‚asnych labelek
GLINER_DEFAULT_MODEL = "urchade/gliner_multi_pii-v1"

# Etykiety NER dla polskich danych osobowych uÅ¼ywanych w zadaniu
# Threshold 0.4 jest celowo niski - lepiej za duÅ¼o cenzury niÅ¼ za maÅ‚o
# MoÅ¼esz podnieÅ›Ä‡ do 0.5 jeÅ›li masz false positives
GLINER_LABELS = [
    "person",           # imiÄ™ + nazwisko
    "age",              # wiek (np. "45 lat", "lat 27")
    "city",             # miasto
    "street address",   # ulica + numer
    "location",         # fallback dla adresÃ³w ktÃ³rych model nie sklasyfikowaÅ‚ jako street
]
GLINER_THRESHOLD = 0.4

parser = argparse.ArgumentParser(description="Cenzura danych (multi-engine + Claude + GLiNER)")
parser.add_argument(
    "--engine",
    choices=["openai", "lmstudio", "anything", "gemini", "claude", "gliner"],
    help="LLM backend to use",
)
parser.add_argument(
    "--gliner-model",
    default=None,
    help=f"Model GLiNER do uÅ¼ycia (domyÅ›lnie: {GLINER_DEFAULT_MODEL})",
)
parser.add_argument(
    "--gliner-threshold",
    type=float,
    default=GLINER_THRESHOLD,
    help=f"PrÃ³g pewnoÅ›ci dla GLiNER (domyÅ›lnie: {GLINER_THRESHOLD})",
)
args = parser.parse_args()


def detect_engine() -> str:
    """Wykrywa silnik LLM na podstawie argumentÃ³w i zmiennych Å›rodowiskowych"""
    if args.engine:
        return args.engine.lower()
    elif os.getenv("LLM_ENGINE"):
        return os.getenv("LLM_ENGINE").lower()
    else:
        # PrÃ³buj wykryÄ‡ silnik na podstawie ustawionych zmiennych MODEL_NAME
        model_name = os.getenv("MODEL_NAME", "")
        if "claude" in model_name.lower():
            return "claude"
        elif "gemini" in model_name.lower():
            return "gemini"
        elif "gpt" in model_name.lower() or "openai" in model_name.lower():
            return "openai"
        else:
            # SprawdÅº ktÃ³re API keys sÄ… dostÄ™pne
            if os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY"):
                return "claude"
            elif os.getenv("GEMINI_API_KEY"):
                return "gemini"
            elif os.getenv("OPENAI_API_KEY"):
                return "openai"
            else:
                return "lmstudio"  # domyÅ›lnie


def validate_engine(engine: str) -> None:
    """Waliduje czy silnik jest obsÅ‚ugiwany"""
    if engine not in {"openai", "lmstudio", "anything", "gemini", "claude", "gliner"}:
        print(f"{UNSUPPORTED_ENGINE_MSG} {engine}", file=sys.stderr)
        sys.exit(1)


def validate_environment() -> None:
    """Sprawdza czy wszystkie wymagane zmienne Å›rodowiskowe sÄ… ustawione"""
    required_vars = ["CENTRALA_API_KEY", "REPORT_URL", "CENZURA_URL"]
    missing_vars = [var for var in required_vars if not os.getenv(var)]

    if missing_vars:
        print(f"âŒ Brak ustawienia: {', '.join(missing_vars)} w .env", file=sys.stderr)
        sys.exit(1)


# Inicjalizacja i walidacja
ENGINE = detect_engine()
validate_engine(ENGINE)
validate_environment()

print(f"ðŸ”„ ENGINE wykryty: {ENGINE}")
print(f"âœ… Engine: {ENGINE}")

CENTRALA_API_KEY = os.getenv("CENTRALA_API_KEY")
REPORT_URL = os.getenv("REPORT_URL")
CENZURA_URL = os.getenv("CENZURA_URL")

# --- ULTRA-TWARDY PROMPT (uÅ¼ywany tylko przez silniki LLM) ---
PROMPT_SYSTEM = (
    "JesteÅ› automatem do cenzury danych osobowych w jÄ™zyku polskim. "
    "NIE WOLNO Ci zmieniaÄ‡ Å¼adnych innych sÅ‚Ã³w, znakÃ³w interpunkcyjnych, ukÅ‚adu tekstu ani zamieniaÄ‡ kolejnoÅ›ci zdaÅ„. "
    "ZamieÅ„ TYLKO i WYÅÄ„CZNIE:\n"
    "- kaÅ¼de imiÄ™ i nazwisko na 'CENZURA',\n"
    "- kaÅ¼dÄ… nazwÄ™ miasta na 'CENZURA',\n"
    "- kaÅ¼dÄ… nazwÄ™ ulicy wraz z numerem domu/mieszkania na 'CENZURA' (prefiks 'ul.' POZOSTAW jeÅ›li jest w tekÅ›cie, np. 'ul. Polna 8' â†’ 'ul. CENZURA'),\n"
    "- kaÅ¼dÄ… informacjÄ™ o wieku (np. '45 lat', 'wiek: 32', 'lat 27', 'ma 29 lat') na 'CENZURA'.\n"
    "Nie wolno parafrazowaÄ‡, nie wolno podsumowywaÄ‡, nie wolno streszczaÄ‡ ani zamieniaÄ‡ kolejnoÅ›ci czegokolwiek. "
    "Wynikowy tekst musi mieÄ‡ identyczny ukÅ‚ad, interpunkcjÄ™ i liczbÄ™ linii jak oryginaÅ‚. "
    "KaÅ¼da inna zmiana niÅ¼ cenzura wyÅ¼ej powoduje bÅ‚Ä…d i NIEZALICZENIE zadania. "
    "Nie pisz Å¼adnych komentarzy, nie wyjaÅ›niaj odpowiedzi. "
    "ODPOWIEDZ WYÅÄ„CZNIE TEKSTEM Z OCENZURÄ„. "
    "PRZYKÅAD:\n"
    "OryginaÅ‚:\n"
    "Dane podejrzanego: Jan Kowalski, lat 45, mieszka w Krakowie, ul. Polna 8.\n"
    "WyjÅ›cie:\n"
    "Dane podejrzanego: CENZURA, lat CENZURA, mieszka w CENZURA, ul. CENZURA."
)


def download_text(url: str) -> str:
    """Pobiera tekst z podanego URL"""
    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        return resp.text.strip()
    except requests.RequestException as e:
        print(f"âŒ BÅ‚Ä…d podczas pobierania danych: {e}", file=sys.stderr)
        sys.exit(1)


# --- KLASY LLM CLIENT ---

class LLMCensorClient(ABC):
    """Bazowa klasa dla klientÃ³w cenzury"""

    def __init__(self, model_name: str):
        self.model_name = model_name

    @abstractmethod
    def censor_text(self, text: str) -> str:
        """Metoda do cenzury tekstu - implementacja w podklasach"""
        pass

    def create_user_prompt(self, text: str) -> str:
        """Tworzy prompt uÅ¼ytkownika"""
        return (
            "Tekst do cenzury (nie zmieniaj nic poza danymi osobowymi, przykÅ‚ad wyÅ¼ej!):\n"
            + text
        )


class GLiNERCensorClient(LLMCensorClient):
    """
    Klient cenzury oparty na GLiNER - deterministyczny NER bez LLM.

    Jak dziaÅ‚a:
    1. Model zwraca listÄ™: [{text, label, start, end, score}, ...]
    2. Sortujemy encje od koÅ„ca tekstu, Å¼eby podmiana nie przesuwaÅ‚a indeksÃ³w
    3. Wycinamy span [start:end] i wstawiamy "CENZURA"
    4. Zero ryzyka zmiany interpunkcji czy reszty tekstu - operujemy na char-offsets

    Instalacja:
        pip install gliner
        # Model pobierze siÄ™ automatycznie z HuggingFace przy pierwszym uruchomieniu
        # (~500MB dla gliner_multi_pii-v1)
    """

    def __init__(
        self,
        model_name: str = GLINER_DEFAULT_MODEL,
        labels: List[str] = None,
        threshold: float = GLINER_THRESHOLD,
    ):
        super().__init__(model_name)
        self.labels = labels or GLINER_LABELS
        self.threshold = threshold
        self._model = None  # lazy loading - Å‚aduj model dopiero przy pierwszym uÅ¼yciu

    def _load_model(self):
        """Åaduje model GLiNER (lazy - tylko raz)"""
        if self._model is not None:
            return

        try:
            from gliner import GLiNER
        except ImportError:
            print(MISSING_GLINER_INSTALL_MSG, file=sys.stderr)
            sys.exit(1)

        print(f"ðŸ”„ Åadowanie modelu GLiNER: {self.model_name}")
        print("   (pierwsze uruchomienie pobiera ~500MB z HuggingFace)")
        self._model = GLiNER.from_pretrained(self.model_name)
        print(f"âœ… Model GLiNER zaÅ‚adowany")

    def _find_entities(self, text: str) -> List[Dict]:
        """Wykrywa encje PII w tekÅ›cie"""
        entities = self._model.predict_entities(text, self.labels, threshold=self.threshold)

        if not entities:
            return []

        # Loguj co wykryto - pomocne przy debugowaniu threshold
        print(f"[ðŸ” GLiNER wykryÅ‚ {len(entities)} encji:]")
        for ent in sorted(entities, key=lambda e: e["start"]):
            print(
                f"   [{ent['label']:20s}] score={ent['score']:.3f} | "
                f"'{ent['text']}' (pos {ent['start']}-{ent['end']})"
            )

        return entities

    # Prefiksy adresowe ktÃ³re naleÅ¼y zachowaÄ‡ przed CENZURA
    # np. "ul. DÅ‚uga 8" â†’ "ul. CENZURA" zamiast "CENZURA"
    STREET_PREFIXES = ("ul. ", "ul.", "al. ", "al.", "pl. ", "pl.", "os. ", "os.")

    def _apply_censorship(self, text: str, entities: List[Dict]) -> str:
        """
        Podmienia wykryte spany na 'CENZURA'.
        Sortuje od koÅ„ca tekstu - podmiana nie przesuwa wczeÅ›niejszych indeksÃ³w.

        Dla encji typu street address zachowuje standardowe prefiksy adresowe
        (ul., al., pl., os.) przed sÅ‚owem CENZURA, zgodnie z oczekiwaniami serwera.
        PrzykÅ‚ad: "ul. DÅ‚uga 8" (pos 50-61) â†’ "ul. CENZURA" a nie "CENZURA".
        """
        # Sortuj od koÅ„ca, Å¼eby indeksy nie "jechaÅ‚y" po podmiance
        entities_sorted = sorted(entities, key=lambda e: e["start"], reverse=True)

        result = text
        for entity in entities_sorted:
            start = entity["start"]
            end = entity["end"]

            # Dla ulic: jeÅ›li span zaczyna siÄ™ od prefiksu adresowego, zachowaj go
            # DziaÅ‚a na oryginalnym tekÅ›cie (result moÅ¼e byÄ‡ juÅ¼ czÄ™Å›ciowo podmieniony,
            # ale sortowanie od koÅ„ca gwarantuje Å¼e wczeÅ›niejsze pozycje sÄ… nienaruszone)
            if entity["label"] in ("street address", "location", "address"):
                span = result[start:end]
                for prefix in self.STREET_PREFIXES:
                    if span.lower().startswith(prefix.lower()):
                        # PrzesuÅ„ start za prefix - cenzurujemy tylko nazwÄ™+numer
                        start += len(prefix)
                        break

            result = result[:start] + "CENZURA" + result[end:]

        return result

    def censor_text(self, text: str) -> str:
        """
        Cenzuruje tekst uÅ¼ywajÄ…c GLiNER NER.
        Deterministyczny - identyczny wynik dla identycznego wejÅ›cia.
        """
        self._load_model()

        entities = self._find_entities(text)

        if not entities:
            print("âš ï¸  GLiNER nie wykryÅ‚ Å¼adnych encji PII!")
            print("    SprÃ³buj obniÅ¼yÄ‡ --gliner-threshold (obecny: {self.threshold})")
            print("    lub uÅ¼yj innego modelu (--gliner-model)")
            return text

        censored = self._apply_censorship(text, entities)
        return censored


class OpenAICensorClient(LLMCensorClient):
    """Klient cenzury dla OpenAI"""

    def __init__(self, model_name: str, api_key: str, base_url: str):
        super().__init__(model_name)
        try:
            from openai import OpenAI
        except ImportError:
            print(MISSING_OPENAI_INSTALL_MSG, file=sys.stderr)
            sys.exit(1)

        self.client = OpenAI(api_key=api_key, base_url=base_url)

    def censor_text(self, text: str) -> str:
        prompt_user = self.create_user_prompt(text)

        resp = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )

        self._log_usage(resp.usage)
        return resp.choices[0].message.content.strip()

    def _log_usage(self, usage: Any) -> None:
        """Loguje uÅ¼ycie tokenÃ³w i koszty dla OpenAI"""
        tokens = usage
        cost = (
            tokens.prompt_tokens / 1_000_000 * 0.60
            + tokens.completion_tokens / 1_000_000 * 2.40
        )
        print(
            f"[ðŸ“Š Prompt: {tokens.prompt_tokens} | "
            f"Completion: {tokens.completion_tokens} | "
            f"Total: {tokens.total_tokens}]"
        )
        print(f"[ðŸ’° Koszt OpenAI: {cost:.6f} USD]")


class ClaudeCensorClient(LLMCensorClient):
    """Klient cenzury dla Claude"""

    def __init__(self, model_name: str, api_key: str):
        super().__init__(model_name)
        try:
            from anthropic import Anthropic
        except ImportError:
            print(MISSING_ANTHROPIC_INSTALL_MSG, file=sys.stderr)
            sys.exit(1)

        self.client = Anthropic(api_key=api_key)

    def censor_text(self, text: str) -> str:
        prompt_user = self.create_user_prompt(text)

        resp = self.client.messages.create(
            model=self.model_name,
            messages=[
                {"role": "user", "content": PROMPT_SYSTEM + "\n\n" + prompt_user}
            ],
            temperature=0,
            max_tokens=4000,
        )

        self._log_usage(resp.usage)
        return resp.content[0].text.strip()

    def _log_usage(self, usage: Any) -> None:
        """Loguje uÅ¼ycie tokenÃ³w i koszty dla Claude"""
        cost = usage.input_tokens * 0.00003 + usage.output_tokens * 0.00015
        print(
            f"[ðŸ“Š Prompt: {usage.input_tokens} | "
            f"Completion: {usage.output_tokens} | "
            f"Total: {usage.input_tokens + usage.output_tokens}]"
        )
        print(f"[ðŸ’° Koszt Claude: {cost:.6f} USD]")


class GeminiCensorClient(LLMCensorClient):
    """Klient cenzury dla Gemini"""

    def __init__(self, model_name: str, api_key: str):
        super().__init__(model_name)
        try:
            import google.generativeai as genai
        except ImportError:
            print(MISSING_GEMINI_INSTALL_MSG, file=sys.stderr)
            sys.exit(1)

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)

    def censor_text(self, text: str) -> str:
        prompt_user = self.create_user_prompt(text)

        response = self.model.generate_content(
            [PROMPT_SYSTEM + "\n" + prompt_user],
            generation_config={"temperature": 0.0, "max_output_tokens": 4096},
        )

        self._log_usage()
        return response.text.strip()

    def _log_usage(self) -> None:
        """Loguje informacje o uÅ¼yciu dla Gemini"""
        print("[ðŸ“Š Gemini - brak szczegÃ³Å‚Ã³w tokenÃ³w]")
        print("[ðŸ’° Gemini - sprawdÅº limity w Google AI Studio]")


class LocalLLMCensorClient(LLMCensorClient):
    """Klient cenzury dla lokalnych modeli (LMStudio, Anything)"""

    def __init__(self, model_name: str, api_key: str, base_url: str, engine_name: str):
        super().__init__(model_name)
        try:
            from openai import OpenAI
        except ImportError:
            print(MISSING_OPENAI_INSTALL_MSG, file=sys.stderr)
            sys.exit(1)

        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.engine_name = engine_name

    def censor_text(self, text: str) -> str:
        prompt_user = self.create_user_prompt(text)

        resp = self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": PROMPT_SYSTEM},
                {"role": "user", "content": prompt_user},
            ],
            temperature=0,
        )

        self._log_usage(resp.usage)
        return resp.choices[0].message.content.strip()

    def _log_usage(self, usage: Any) -> None:
        """Loguje uÅ¼ycie tokenÃ³w dla lokalnych modeli"""
        tokens = usage
        print(
            f"[ðŸ“Š Prompt: {tokens.prompt_tokens} | "
            f"Completion: {tokens.completion_tokens} | "
            f"Total: {tokens.total_tokens}]"
        )
        print("[ðŸ’° Model lokalny - brak kosztÃ³w]")


def create_censor_client() -> LLMCensorClient:
    """Factory function dla tworzenia klienta cenzury"""

    if ENGINE == "gliner":
        # Parametry GLiNER moÅ¼na nadpisaÄ‡ przez CLI (--gliner-model, --gliner-threshold)
        # lub przez zmienne Å›rodowiskowe GLINER_MODEL, GLINER_THRESHOLD
        model_name = (
            args.gliner_model
            or os.getenv("GLINER_MODEL", GLINER_DEFAULT_MODEL)
        )
        threshold = float(
            os.getenv("GLINER_THRESHOLD", str(args.gliner_threshold))
        )
        print(f"[ðŸ”¬ GLiNER model: {model_name} | threshold: {threshold}]")
        return GLiNERCensorClient(model_name=model_name, threshold=threshold)

    elif ENGINE == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print(MISSING_OPENAI_KEY_MSG, file=sys.stderr)
            sys.exit(1)

        base_url = os.getenv("OPENAI_API_URL", "https://api.openai.com/v1")
        model_name = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_OPENAI", "gpt-4o-mini")
        return OpenAICensorClient(model_name, api_key, base_url)

    elif ENGINE == "claude":
        api_key = os.getenv("CLAUDE_API_KEY") or os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print(MISSING_CLAUDE_KEY_MSG, file=sys.stderr)
            sys.exit(1)

        model_name = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_CLAUDE", "claude-sonnet-4-20250514")
        return ClaudeCensorClient(model_name, api_key)

    elif ENGINE == "gemini":
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            print(MISSING_GEMINI_KEY_MSG, file=sys.stderr)
            sys.exit(1)

        model_name = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_GEMINI", "gemini-2.5-pro-latest")
        return GeminiCensorClient(model_name, api_key)

    elif ENGINE == "lmstudio":
        api_key = os.getenv("LMSTUDIO_API_KEY", "local")
        base_url = os.getenv("LMSTUDIO_API_URL", "http://localhost:1234/v1")
        model_name = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_LM", "llama-3.3-70b-instruct")
        return LocalLLMCensorClient(model_name, api_key, base_url, "LMStudio")

    elif ENGINE == "anything":
        api_key = os.getenv("ANYTHING_API_KEY", "local")
        base_url = os.getenv("ANYTHING_API_URL", "http://localhost:1234/v1")
        model_name = os.getenv("MODEL_NAME") or os.getenv("MODEL_NAME_ANY", "llama-3.3-70b-instruct")
        return LocalLLMCensorClient(model_name, api_key, base_url, "Anything")

    else:
        print(f"âŒ Nieznany silnik: {ENGINE}", file=sys.stderr)
        sys.exit(1)


def censor_llm(text: str) -> str:
    """
    Cenzuruje tekst uÅ¼ywajÄ…c wybranego silnika (LLM lub GLiNER).
    GLiNER: deterministyczny NER, podmiana na char-offsets.
    LLM: instrukcja w prompt, model podmienia sÅ‚ownie.
    """
    client = create_censor_client()
    return client.censor_text(text)


def extract_flag(text: str) -> str:
    """WyciÄ…ga flagÄ™ z tekstu"""
    flag_match = re.search(r"\{\{FLG:[^}]+\}\}|FLG\{[^}]+\}", text)
    return flag_match.group(0) if flag_match else ""


def send_result(censored_text: str) -> None:
    """WysyÅ‚a ocenzurowany tekst do serwera"""
    payload = {"task": "CENZURA", "apikey": CENTRALA_API_KEY, "answer": censored_text}

    try:
        response = requests.post(REPORT_URL, json=payload, timeout=10)
        if response.ok:
            resp_text = response.text.strip()
            flag = extract_flag(resp_text) or extract_flag(censored_text)
            if flag:
                print(flag)
            else:
                print("Brak flagi w odpowiedzi serwera. OdpowiedÅº:", resp_text)
        else:
            print(f"âŒ BÅ‚Ä…d HTTP {response.status_code}: {response.text}", file=sys.stderr)
    except requests.RequestException as e:
        print(f"âŒ BÅ‚Ä…d podczas wysyÅ‚ania danych: {e}", file=sys.stderr)
        sys.exit(1)


def main() -> None:
    """GÅ‚Ã³wna funkcja programu"""
    raw_text = download_text(CENZURA_URL)
    print(f"ðŸ”„ Pobrano tekst ({len(raw_text)} znakÃ³w)")
    print(f"ðŸ”„ CenzurujÄ™ uÅ¼ywajÄ…c {ENGINE}...")

    censored_text = censor_llm(raw_text)
    print("=== OCENZUROWANY OUTPUT ===")
    print(censored_text)
    print("===========================")

    send_result(censored_text)


if __name__ == "__main__":
    main()
